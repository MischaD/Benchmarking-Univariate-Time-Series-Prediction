{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Analysis of the memory consumption of the Transformer given the maximum sequence length of 1024\n",
    "\n",
    "This notebook looks at how high we can make the batch size if all the other hyperparameters that influence\n",
    "the memory consumption are fixed to a value.\n",
    "\n",
    "Most of the default values are taken from AIAYN with a lower value for d_model.\n",
    "Please look at the report for more details.\n",
    "\n",
    "~ 6.2 GiB GPU Memory\n",
    "\n",
    "## Load Transformer model and initialize Hyperparameters with little to no impact on the memory consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from src.models.transformer import make_time_series_model, make_encoder_to_decoder_time_series_model, make_encoder_only_time_series_model\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Hyperparameters with no significant influence on the memory consumption\n",
    "epochs = 3\n",
    "# Model Parameters\n",
    "dropout = 0.2  # Dropout rate\n",
    "debug = False # only 1 batch per epoch\n",
    "d_input = 1 # From dataset\n",
    "d_output = 1  # From dataset\n",
    "\n",
    "# Loss function for gradient descent, possible choices are SL1, MSE\n",
    "loss_function_ = \"SL1\"\n",
    "device = torch.device(\"cuda:0\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define some helper functions to try and run the model with increasingly higher sizes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def test_model_memory_consumption(model, d_model, N, h, d_ff, sequence_len, batch_size):\n",
    "    \"\"\" Test whether the Transformer model fits on the current system regarding its memory consumption.\n",
    "\n",
    "    :param model: (str) {'tst': normal transformer, 'encoder_only_tst': encoder only architecture, 'enc2dec_tst': encoder2decoder transformer\n",
    "    :param d_model: (int) model dimensionality\n",
    "    :param N: (int) Number of layers\n",
    "    :param h: (int) Number of heads in a single layer\n",
    "    :param d_ff: (int) Dimensionality of the positionwise feedforward network\n",
    "    :param sequence_len: (int) length of the input and output seqeuence\n",
    "    :param batch_size: (int) batch size to be tested\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global d_input, d_output, dropout\n",
    "    train_data = torch.rand((batch_size, sequence_len, d_input))\n",
    "    train_data_out = torch.zeros_like(train_data)\n",
    "    y_in = torch.zeros_like(train_data)\n",
    "    x_mask = torch.ones(1, 1, sequence_len).to(device)\n",
    "    y_mask = torch.ones_like(x_mask)\n",
    "    loss_function = nn.SmoothL1Loss()\n",
    "\n",
    "    if model == \"tst\":\n",
    "        model = make_time_series_model(d_input=d_input,\n",
    "                                   d_output=d_output,\n",
    "                                   N=N,\n",
    "                                   d_model=d_model,\n",
    "                                   d_ff=d_ff,\n",
    "                                   h=h,\n",
    "                                   dropout=dropout,\n",
    "                                   device=device)\n",
    "        forward_pass_args = (train_data.to(device), y_in.to(device), x_mask, y_mask)\n",
    "    elif model == \"encoder_only_tst\":\n",
    "        model = make_encoder_only_time_series_model(d_input=d_input,\n",
    "                                   d_output=d_output,\n",
    "                                   N=N,\n",
    "                                   d_model=d_model,\n",
    "                                   d_ff=d_ff,\n",
    "                                   h=h,\n",
    "                                   dropout=dropout,\n",
    "                                   device=device)\n",
    "        forward_pass_args = (train_data.to(device), x_mask)\n",
    "    elif model == \"enc2dec_tst\":\n",
    "        model = make_encoder_to_decoder_time_series_model(d_input=d_input,\n",
    "                                   d_output=d_output,\n",
    "                                   N=N,\n",
    "                                   d_model=d_model,\n",
    "                                   d_ff=d_ff,\n",
    "                                   h=h,\n",
    "                                   dropout=dropout,\n",
    "                                   device=device)\n",
    "        forward_pass_args = (train_data.to(device), x_mask)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    # Run two epochs to see if a Runtime Error is thrown\n",
    "    for idx_epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_out_pred = model(*forward_pass_args)\n",
    "\n",
    "        loss = loss_function(train_data_out.to(device), y_out_pred)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "\n",
    "def find_maximum_batch_size(d_model, N, h, d_ff, sequence_len):\n",
    "    \"\"\" Given some parameters about the Transformer model that have a (presumably) high influence on its memeory consumption\n",
    "    empirically determine the largest possible batch size (as a power of 2) that still runs on a single GPU\n",
    "    (~ 6GiB of used memory). The test is done for all types of Transformer architectures (normal, decoder_only, encoder2decoder)\n",
    "\n",
    "    :param d_model: (int) model dimensionality\n",
    "    :param N: (int) layer depth\n",
    "    :param h: (int) Number of attention heads\n",
    "    :param d_ff: (int) pointwise feed forward network\n",
    "    :param sequence_len: (int) input and output sequence length\n",
    "    :return: list of maximum batch sizes for the three architectures in the order ['normal', 'decoder_only', 'encoder2decoder']\n",
    "    \"\"\"\n",
    "    models = [\"tst\", \"encoder_only_tst\", \"enc2dec_tst\"]\n",
    "    for model in models:\n",
    "        print(f\"Model: {model} - Finding maximum batch size for model with d_model: {d_model}, N:{N}, d_ff:{d_ff}, sequence_len:{sequence_len}\")\n",
    "        out_of_memory = False\n",
    "        batch_size = 1\n",
    "        while not out_of_memory:\n",
    "            try:\n",
    "                test_model_memory_consumption(model=model, d_model=d_model, N=N, h=h, d_ff=d_ff, sequence_len=sequence_len, batch_size=batch_size)\n",
    "            except RuntimeError:\n",
    "                out_of_memory = True\n",
    "                batch_size = batch_size // 2 # last successful run\n",
    "            else:\n",
    "                batch_size *= 2\n",
    "\n",
    "        print(f\"Maximum Batch Size: {batch_size}\")\n",
    "        print(f\"Estimated maximum batch size if we use Log-Sparse:{int(batch_size * sequence_len / (np.log(sequence_len)**2))}\")\n",
    "        print(\"-\"*50)\n",
    "    print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Hyperparameters that have a high influence on the memory consumption and calculate how large we could make the Batch size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: tst - Finding maximum batch size for model with d_model: 64, N:8, d_ff:128, sequence_len:256\n",
      "Maximum Batch Size: 32\n",
      "Estimated maximum batch size if we use Log-Sparse:266\n",
      "--------------------------------------------------\n",
      "Model: encoder_only_tst - Finding maximum batch size for model with d_model: 64, N:8, d_ff:128, sequence_len:256\n",
      "Maximum Batch Size: 64\n",
      "Estimated maximum batch size if we use Log-Sparse:532\n",
      "--------------------------------------------------\n",
      "Model: enc2dec_tst - Finding maximum batch size for model with d_model: 64, N:8, d_ff:128, sequence_len:256\n",
      "Maximum Batch Size: 32\n",
      "Estimated maximum batch size if we use Log-Sparse:266\n",
      "--------------------------------------------------\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "d_model = 64 # Latent dim\n",
    "N = 8  # Number of encoder and decoder to stack\n",
    "h = 8  # Number of heads\n",
    "d_ff = 128\n",
    "sequence_len = 256\n",
    "\n",
    "find_maximum_batch_size(d_model=d_model, N=N, h=h, d_ff=d_ff, sequence_len=sequence_len)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model dimensionality has less impact than what I expected. The reason for this is described in the report.\n",
    "\n",
    "The impact of N, h, the type of architecture and sequence_len is significant"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Interesting setting:\n",
    "\n",
    "d_model = 64 # Latent dim\n",
    "N = 6  # Number of encoder and decoder to stack\n",
    "h = 8  # Number of heads\n",
    "d_ff = 1024\n",
    "sequence_len = 1024\n",
    "\n",
    "Works for batch_size = 8 of the decoder only Transformer\n",
    "Doesn't scale"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}